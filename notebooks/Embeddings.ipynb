{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f9ce07",
   "metadata": {},
   "source": [
    "### Embedding and Vector indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522f9c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Embeddings · Step A: Load & sanity-check ===\n",
      "Looking for: /Users/valentinreateguirangel/Documents/MSc Machine Learning/Finance_RAG_why_move/finance-rag-why-move/data/news_clean.csv\n",
      "Loaded rows: 887,221\n",
      "Columns: ['date', 'title', 'source', 'doc_id']\n",
      "Null titles:  0\n",
      "Null sources: 0\n",
      "Duplicate doc_id values: 0\n",
      "\n",
      "Preview (5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>Agilent Technologies Announces Pricing of $5……...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>news_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>Agilent (A) Gears Up for Q2 Earnings: What's i...</td>\n",
       "      <td>Zacks</td>\n",
       "      <td>news_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>J.P. Morgan Asset Management Announces Liquida...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>news_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>Pershing Square Capital Management, L.P. Buys ...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>news_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>Agilent Awards Trilogy Sciences with a Golden ...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>news_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                              title     source  \\\n",
       "0  2020-06-01  Agilent Technologies Announces Pricing of $5……...  GuruFocus   \n",
       "1  2020-05-18  Agilent (A) Gears Up for Q2 Earnings: What's i...      Zacks   \n",
       "2  2020-05-15  J.P. Morgan Asset Management Announces Liquida...  GuruFocus   \n",
       "3  2020-05-15  Pershing Square Capital Management, L.P. Buys ...  GuruFocus   \n",
       "4  2020-05-12  Agilent Awards Trilogy Sciences with a Golden ...  GuruFocus   \n",
       "\n",
       "   doc_id  \n",
       "0  news_0  \n",
       "1  news_1  \n",
       "2  news_2  \n",
       "3  news_3  \n",
       "4  news_4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load news_clean.csv and sanity-check\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== Embeddings · Step A: Load & sanity-check ===\")\n",
    "\n",
    "DATA_DIR = Path(\"../data\").resolve()\n",
    "news_path = DATA_DIR / \"news_clean.csv\"\n",
    "\n",
    "print(f\"Looking for: {news_path}\")\n",
    "if not news_path.exists():\n",
    "    raise FileNotFoundError(f\"Could not find {news_path}. Did you run the news cleaning notebook?\")\n",
    "\n",
    "df = pd.read_csv(news_path)\n",
    "\n",
    "print(f\"Loaded rows: {len(df):,}\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "required = [\"date\", \"title\", \"source\", \"doc_id\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise AssertionError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# Light cleanup (no mutation yet)\n",
    "null_title = df[\"title\"].isna().sum()\n",
    "null_source = df[\"source\"].isna().sum()\n",
    "dupe_ids = df[\"doc_id\"].duplicated().sum()\n",
    "\n",
    "print(f\"Null titles:  {null_title:,}\")\n",
    "print(f\"Null sources: {null_source:,}\")\n",
    "print(f\"Duplicate doc_id values: {dupe_ids:,}\")\n",
    "\n",
    "print(\"\\nPreview (5 rows):\")\n",
    "display(df.head(5)[[\"date\", \"title\", \"source\", \"doc_id\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ca4b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Embeddings · Step B: model + paths ===\n",
      "Device selected: mps\n",
      "Loading model: sentence-transformers/all-MiniLM-L6-v2 ...\n",
      "Model loaded.\n",
      "Embedding dim: 384 (expected 384)\n",
      "Chroma index directory: /Users/valentinreateguirangel/Documents/MSc Machine Learning/Finance_RAG_why_move/finance-rag-why-move/data/chroma_index/why-move-v1\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model and prepare Chroma index directory\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"=== Embeddings · Step B: model + paths ===\")\n",
    "\n",
    "#  Choose device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Silicon GPU\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Device selected: {device}\")\n",
    "\n",
    "#  Load model\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"Loading model: {MODEL_NAME} ...\")\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# quick embedding smoke test (tiny)\n",
    "test_vec = model.encode([\"hello world\"], normalize_embeddings=True)\n",
    "assert test_vec.shape == (1, 384), f\"Unexpected embedding shape: {test_vec.shape}\"\n",
    "print(f\"Embedding dim: {test_vec.shape[1]} (expected 384)\")\n",
    "\n",
    "# Prepare Chroma output directory\n",
    "INDEX_DIR = Path(\"../data/chroma_index/why-move-v1\").resolve()\n",
    "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Chroma index directory: {INDEX_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9595a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test insert (clean) — 100 titles into Chroma ===\n",
      "Rows selected for test: 100\n",
      "Encoding test titles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (100, 384)\n",
      "Collection ready: why-move-v1\n",
      "✅ Test upsert OK. Collection count now: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valentinreateguirangel/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:10<00:00, 8.05MiB/s]\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample query: Agilent Technologies Announces Pricing of $5…… M\n",
      "Top matches:\n",
      " - Agilent Technologies Announces Pricing of $5…… Million of Senior Notes\n",
      " - Agilent Technologies (A) Surpasses Q4 Earnings and Revenue Estimates\n",
      " - Agilent Technologies (A) Earnings Expected to Grow: Should You Buy?\n",
      "\n",
      "Note: This was a 100-row smoke test. Next we’ll batch through all rows safely.\n"
     ]
    }
   ],
   "source": [
    "# Embed first 100 titles and upsert into Chroma (v0.5+)\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions  # (not required, but handy later)\n",
    "\n",
    "print(\"=== Test insert (clean) — 100 titles into Chroma ===\")\n",
    "\n",
    "#  Slice a safe test batch\n",
    "df_test = df.head(100).copy()\n",
    "assert {\"title\", \"doc_id\", \"date\", \"source\"}.issubset(df_test.columns), \"Missing required columns\"\n",
    "df_test[\"title\"] = df_test[\"title\"].astype(str).str.strip()\n",
    "df_test = df_test[df_test[\"title\"] != \"\"]\n",
    "print(f\"Rows selected for test: {len(df_test)}\")\n",
    "\n",
    "#  Prepare inputs\n",
    "texts = df_test[\"title\"].tolist()\n",
    "ids = [f\"{i}-t\" for i in df_test[\"doc_id\"].tolist()]  # '-t' suffix to mark test inserts\n",
    "metas = df_test[[\"date\", \"source\", \"doc_id\"]].to_dict(orient=\"records\")\n",
    "\n",
    "#  Embed (normalize for cosine similarity)\n",
    "print(\"Encoding test titles...\")\n",
    "embs = model.encode(texts, normalize_embeddings=True).tolist()\n",
    "print(f\"Embeddings shape: ({len(embs)}, {len(embs[0]) if embs else '??'})\")\n",
    "\n",
    "#  Connect to persistent Chroma collection\n",
    "COLLECTION_NAME = \"why-move-v1\"\n",
    "client = chromadb.PersistentClient(path=str(INDEX_DIR))\n",
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "print(f\"Collection ready: {collection.name}\")\n",
    "\n",
    "# Upsert\n",
    "collection.upsert(\n",
    "    ids=ids,\n",
    "    documents=texts,\n",
    "    embeddings=embs,\n",
    "    metadatas=metas,\n",
    ")\n",
    "\n",
    "# Confirm\n",
    "count = collection.count()\n",
    "print(f\" Test upsert OK. Collection count now: {count}\")\n",
    "\n",
    "# Quick query sanity check\n",
    "q = texts[0][:48]\n",
    "res = collection.query(query_texts=[q], n_results=3)\n",
    "print(\"\\nSample query:\", q)\n",
    "print(\"Top matches:\")\n",
    "for t in res.get(\"documents\", [[]])[0]:\n",
    "    print(\" -\", (t[:80] + \"...\") if len(t) > 80 else t)\n",
    "\n",
    "print(\"\\nNote: This was a 100-row smoke test. Next we’ll batch through all rows safely.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1c99a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch embedding & upsert into Chroma ===\n",
      "Total rows considered this run: 887,221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after stripping empty titles: 887,221\n",
      "Collection: why-move-v1\n",
      "Processing from index 0 in 888 batches of 1000.\n",
      "  - Up to row 10,000 / 887,221 (this run). Collection count: 10100\n",
      "  - Up to row 20,000 / 887,221 (this run). Collection count: 20100\n",
      "  - Up to row 30,000 / 887,221 (this run). Collection count: 30100\n",
      "  - Up to row 40,000 / 887,221 (this run). Collection count: 40100\n",
      "  - Up to row 50,000 / 887,221 (this run). Collection count: 50100\n",
      "  - Up to row 60,000 / 887,221 (this run). Collection count: 60100\n",
      "  - Up to row 70,000 / 887,221 (this run). Collection count: 70100\n",
      "  - Up to row 80,000 / 887,221 (this run). Collection count: 80100\n",
      "  - Up to row 90,000 / 887,221 (this run). Collection count: 90100\n",
      "  - Up to row 100,000 / 887,221 (this run). Collection count: 100100\n",
      "  - Up to row 110,000 / 887,221 (this run). Collection count: 110100\n",
      "  - Up to row 120,000 / 887,221 (this run). Collection count: 120100\n",
      "  - Up to row 130,000 / 887,221 (this run). Collection count: 130100\n",
      "  - Up to row 140,000 / 887,221 (this run). Collection count: 140100\n",
      "  - Up to row 150,000 / 887,221 (this run). Collection count: 150100\n",
      "  - Up to row 160,000 / 887,221 (this run). Collection count: 160100\n",
      "  - Up to row 170,000 / 887,221 (this run). Collection count: 170100\n",
      "  - Up to row 180,000 / 887,221 (this run). Collection count: 180100\n",
      "  - Up to row 190,000 / 887,221 (this run). Collection count: 190100\n",
      "  - Up to row 200,000 / 887,221 (this run). Collection count: 200100\n",
      "  - Up to row 210,000 / 887,221 (this run). Collection count: 210100\n",
      "  - Up to row 220,000 / 887,221 (this run). Collection count: 220100\n",
      "  - Up to row 230,000 / 887,221 (this run). Collection count: 230100\n",
      "  - Up to row 240,000 / 887,221 (this run). Collection count: 240100\n",
      "  - Up to row 250,000 / 887,221 (this run). Collection count: 250100\n",
      "  - Up to row 260,000 / 887,221 (this run). Collection count: 260100\n",
      "  - Up to row 270,000 / 887,221 (this run). Collection count: 270100\n",
      "  - Up to row 280,000 / 887,221 (this run). Collection count: 280100\n",
      "  - Up to row 290,000 / 887,221 (this run). Collection count: 290100\n",
      "  - Up to row 300,000 / 887,221 (this run). Collection count: 300100\n",
      "  - Up to row 310,000 / 887,221 (this run). Collection count: 310100\n",
      "  - Up to row 320,000 / 887,221 (this run). Collection count: 320100\n",
      "  - Up to row 330,000 / 887,221 (this run). Collection count: 330100\n",
      "  - Up to row 340,000 / 887,221 (this run). Collection count: 340100\n",
      "  - Up to row 350,000 / 887,221 (this run). Collection count: 350100\n",
      "  - Up to row 360,000 / 887,221 (this run). Collection count: 360100\n",
      "  - Up to row 370,000 / 887,221 (this run). Collection count: 370100\n",
      "  - Up to row 380,000 / 887,221 (this run). Collection count: 380100\n",
      "  - Up to row 390,000 / 887,221 (this run). Collection count: 390100\n",
      "  - Up to row 400,000 / 887,221 (this run). Collection count: 400100\n",
      "  - Up to row 410,000 / 887,221 (this run). Collection count: 410100\n",
      "  - Up to row 420,000 / 887,221 (this run). Collection count: 420100\n",
      "  - Up to row 430,000 / 887,221 (this run). Collection count: 430100\n",
      "  - Up to row 440,000 / 887,221 (this run). Collection count: 440100\n",
      "  - Up to row 450,000 / 887,221 (this run). Collection count: 450100\n",
      "  - Up to row 460,000 / 887,221 (this run). Collection count: 460100\n",
      "  - Up to row 470,000 / 887,221 (this run). Collection count: 470100\n",
      "  - Up to row 480,000 / 887,221 (this run). Collection count: 480100\n",
      "  - Up to row 490,000 / 887,221 (this run). Collection count: 490100\n",
      "  - Up to row 500,000 / 887,221 (this run). Collection count: 500100\n",
      "  - Up to row 510,000 / 887,221 (this run). Collection count: 510100\n",
      "  - Up to row 520,000 / 887,221 (this run). Collection count: 520100\n",
      "  - Up to row 530,000 / 887,221 (this run). Collection count: 530100\n",
      "  - Up to row 540,000 / 887,221 (this run). Collection count: 540100\n",
      "  - Up to row 550,000 / 887,221 (this run). Collection count: 550100\n",
      "  - Up to row 560,000 / 887,221 (this run). Collection count: 560100\n",
      "  - Up to row 570,000 / 887,221 (this run). Collection count: 570100\n",
      "  - Up to row 580,000 / 887,221 (this run). Collection count: 580100\n",
      "  - Up to row 590,000 / 887,221 (this run). Collection count: 590100\n",
      "  - Up to row 600,000 / 887,221 (this run). Collection count: 600100\n",
      "  - Up to row 610,000 / 887,221 (this run). Collection count: 610100\n",
      "  - Up to row 620,000 / 887,221 (this run). Collection count: 620100\n",
      "  - Up to row 630,000 / 887,221 (this run). Collection count: 630100\n",
      "  - Up to row 640,000 / 887,221 (this run). Collection count: 640100\n",
      "  - Up to row 650,000 / 887,221 (this run). Collection count: 650100\n",
      "  - Up to row 660,000 / 887,221 (this run). Collection count: 660100\n",
      "  - Up to row 670,000 / 887,221 (this run). Collection count: 670100\n",
      "  - Up to row 680,000 / 887,221 (this run). Collection count: 680100\n",
      "  - Up to row 690,000 / 887,221 (this run). Collection count: 690100\n",
      "  - Up to row 700,000 / 887,221 (this run). Collection count: 700100\n",
      "  - Up to row 710,000 / 887,221 (this run). Collection count: 710100\n",
      "  - Up to row 720,000 / 887,221 (this run). Collection count: 720100\n",
      "  - Up to row 730,000 / 887,221 (this run). Collection count: 730100\n",
      "  - Up to row 740,000 / 887,221 (this run). Collection count: 740100\n",
      "  - Up to row 750,000 / 887,221 (this run). Collection count: 750100\n",
      "  - Up to row 760,000 / 887,221 (this run). Collection count: 760100\n",
      "  - Up to row 770,000 / 887,221 (this run). Collection count: 770100\n",
      "  - Up to row 780,000 / 887,221 (this run). Collection count: 780100\n",
      "  - Up to row 790,000 / 887,221 (this run). Collection count: 790100\n",
      "  - Up to row 800,000 / 887,221 (this run). Collection count: 800100\n",
      "  - Up to row 810,000 / 887,221 (this run). Collection count: 810100\n",
      "  - Up to row 820,000 / 887,221 (this run). Collection count: 820100\n",
      "  - Up to row 830,000 / 887,221 (this run). Collection count: 830100\n",
      "  - Up to row 840,000 / 887,221 (this run). Collection count: 840100\n",
      "  - Up to row 850,000 / 887,221 (this run). Collection count: 850100\n",
      "  - Up to row 860,000 / 887,221 (this run). Collection count: 860100\n",
      "  - Up to row 870,000 / 887,221 (this run). Collection count: 870100\n",
      "  - Up to row 880,000 / 887,221 (this run). Collection count: 880100\n",
      "  - Up to row 887,221 / 887,221 (this run). Collection count: 887321\n",
      "\n",
      " Done. Total processed this run: 887,221\n",
      "Chroma collection count now: 887,321\n",
      "Manifest written to: /Users/valentinreateguirangel/Documents/MSc Machine Learning/Finance_RAG_why_move/finance-rag-why-move/data/index_manifest.json\n"
     ]
    }
   ],
   "source": [
    "#  Batch-embed all titles and upsert into Chroma (resume-friendly)\n",
    "\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== Batch embedding & upsert into Chroma ===\")\n",
    "\n",
    "# --- Config (tweak these safely) ---\n",
    "COLLECTION_NAME = \"why-move-v1\"\n",
    "BATCH_SIZE = 1000          # adjust based on your machine (memory vs speed)\n",
    "MAX_ROWS = None            # e.g., 100_000 for a partial run; None = all rows\n",
    "START_IDX = 0              # set >0 to resume from a later row\n",
    "\n",
    "# --- Input dataframe checks ---\n",
    "assert {\"title\",\"doc_id\",\"date\",\"source\"}.issubset(df.columns), \"df missing required columns\"\n",
    "df_work = df.copy()\n",
    "if MAX_ROWS is not None:\n",
    "    df_work = df_work.iloc[:MAX_ROWS]\n",
    "\n",
    "total_rows = len(df_work)\n",
    "print(f\"Total rows considered this run: {total_rows:,}\")\n",
    "assert total_rows > 0, \"No rows to process.\"\n",
    "\n",
    "# Clean titles and drop empties\n",
    "df_work[\"title\"] = df_work[\"title\"].astype(str).str.strip()\n",
    "df_work = df_work[df_work[\"title\"] != \"\"]\n",
    "total_rows = len(df_work)\n",
    "print(f\"Rows after stripping empty titles: {total_rows:,}\")\n",
    "\n",
    "# Make sure doc_id uniqueness\n",
    "dupes = df_work[\"doc_id\"].duplicated().sum()\n",
    "if dupes:\n",
    "    print(f\"[WARN] Found {dupes} duplicate doc_id rows; keeping first occurrences.\")\n",
    "    df_work = df_work.drop_duplicates(subset=[\"doc_id\"])\n",
    "    total_rows = len(df_work)\n",
    "\n",
    "# --- Connect Chroma persistent collection ---\n",
    "client = chromadb.PersistentClient(path=str(INDEX_DIR))\n",
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "print(f\"Collection: {collection.name}\")\n",
    "\n",
    "# --- Batching ---\n",
    "num_batches = math.ceil((total_rows - START_IDX) / BATCH_SIZE) if total_rows > START_IDX else 0\n",
    "print(f\"Processing from index {START_IDX:,} in {num_batches} batches of {BATCH_SIZE}.\")\n",
    "\n",
    "processed = 0\n",
    "for i in range(START_IDX, total_rows, BATCH_SIZE):\n",
    "    j = min(i + BATCH_SIZE, total_rows)\n",
    "    batch = df_work.iloc[i:j]\n",
    "\n",
    "    texts = batch[\"title\"].tolist()\n",
    "    ids   = batch[\"doc_id\"].tolist()\n",
    "    metas = batch[[\"date\",\"source\",\"doc_id\"]].to_dict(orient=\"records\")\n",
    "\n",
    "    # Embed — normalize for cosine similarity\n",
    "    embs = model.encode(texts, normalize_embeddings=True)\n",
    "    if isinstance(embs, list):\n",
    "        embs = np.asarray(embs, dtype=np.float32)\n",
    "    assert embs.shape[0] == len(batch), f\"Embedding count mismatch at [{i}:{j}]\"\n",
    "\n",
    "    # Upsert into collection (idempotent for re-runs)\n",
    "    collection.upsert(\n",
    "        ids=ids,\n",
    "        documents=texts,\n",
    "        embeddings=embs.tolist(),\n",
    "        metadatas=metas,\n",
    "    )\n",
    "\n",
    "    processed += len(batch)\n",
    "    if ((i // BATCH_SIZE) + 1) % 10 == 0 or j == total_rows:\n",
    "        print(f\"  - Up to row {j:,} / {total_rows:,} (this run). Collection count: {collection.count()}\")\n",
    "\n",
    "print(f\"\\n Done. Total processed this run: {processed:,}\")\n",
    "print(f\"Chroma collection count now: {collection.count():,}\")\n",
    "\n",
    "# --- Write a tiny manifest for reproducibility ---\n",
    "manifest = {\n",
    "    \"collection\": COLLECTION_NAME,\n",
    "    \"persist_directory\": str(INDEX_DIR),\n",
    "    \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"created_or_updated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"rows_indexed_this_run\": processed,\n",
    "    \"total_rows_input_this_run\": total_rows,\n",
    "    \"schema\": [\"date\",\"title\",\"source\",\"doc_id\"],\n",
    "}\n",
    "manifest_path = Path(\"../data/index_manifest.json\").resolve()\n",
    "with open(manifest_path, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print(f\"Manifest written to: {manifest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc145e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (finance-rag)",
   "language": "python",
   "name": "finance-rag-311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
